{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9662342,"sourceType":"datasetVersion","datasetId":5903490}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pprint import pprint\nimport pandas as pd\nimport numpy as np\nimport gc\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:08:46.272865Z","iopub.execute_input":"2024-10-22T05:08:46.273173Z","iopub.status.idle":"2024-10-22T05:08:47.119058Z","shell.execute_reply.started":"2024-10-22T05:08:46.273142Z","shell.execute_reply":"2024-10-22T05:08:47.118244Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # Data Directory\n# data_dir = 'dataset/'\n\n# # kaggle Data Directory\ndata_dir = '/kaggle/input/train-test/dataset/'\n\n# Load the data\ntrain_1 = pd.read_csv(data_dir + 'train_1.csv')\ntrain_2_1 = pd.read_csv(data_dir + 'train_2_1.csv')\ntrain_2_2 = pd.read_csv(data_dir + 'train_2_2.csv')\ntest_1 = pd.read_csv(data_dir + 'test_1.csv')\ntest_2_1 = pd.read_csv(data_dir + 'test_2_1.csv')\ntest_2_2 = pd.read_csv(data_dir + 'test_2_2.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:15:34.820113Z","iopub.execute_input":"2024-10-22T05:15:34.820934Z","iopub.status.idle":"2024-10-22T05:15:59.486374Z","shell.execute_reply.started":"2024-10-22T05:15:34.820893Z","shell.execute_reply":"2024-10-22T05:15:59.485502Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_296/197230518.py:8: DtypeWarning: Columns (126,128,143) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_1 = pd.read_csv(data_dir + 'train_1.csv')\n/tmp/ipykernel_296/197230518.py:9: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_2_1 = pd.read_csv(data_dir + 'train_2_1.csv')\n/tmp/ipykernel_296/197230518.py:10: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_2_2 = pd.read_csv(data_dir + 'train_2_2.csv')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 1: Remove duplicates based on 'id' in the additional information DataFrames\ntrain_2_1 = train_2_1.drop_duplicates(subset='id')\ntrain_2_2 = train_2_2.drop_duplicates(subset='id')\ntest_2_1 = test_2_1.drop_duplicates(subset='id')\ntest_2_2 = test_2_2.drop_duplicates(subset='id')\n\n# Step 2: Identify loans with and without bureau data\n# Create 'train_with_bureau': Loans present in both train_1, train_2_1, and train_2_2\ntrain_with_bureau = pd.merge(train_1, train_2_1, on='id', how='inner')\ntrain_with_bureau = pd.merge(train_with_bureau, train_2_2, on='id', how='inner')\n\n# Create 'train_without_bureau': Loans present only in train_1 (no additional bureau data)\ntrain_without_bureau = train_1[~train_1['id'].isin(train_with_bureau['id'])].copy()\n\n# Step 3: Merge the test DataFrames similarly to how you handled train_with_bureau\n# Create 'test_with_bureau': Loans present in both test_1, test_2_1, and test_2_2\ntest_with_bureau = pd.merge(test_1, test_2_1, on='id', how='inner')\ntest_with_bureau = pd.merge(test_with_bureau, test_2_2, on='id', how='inner')\n\n# Create 'test_without_bureau': Loans present only in test_1 (no additional bureau data)\ntest_without_bureau = test_1[~test_1['id'].isin(test_with_bureau['id'])].copy()\n\n# Step 4: Replace \"NR\" with NaN in both datasets\ntrain_with_bureau.replace(\"NR\", np.nan, inplace=True)\ntrain_without_bureau.replace(\"NR\", np.nan, inplace=True)\ntest_with_bureau.replace(\"NR\", np.nan, inplace=True)\ntest_without_bureau.replace(\"NR\", np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:22.995406Z","iopub.execute_input":"2024-10-22T05:16:22.995796Z","iopub.status.idle":"2024-10-22T05:16:27.108982Z","shell.execute_reply.started":"2024-10-22T05:16:22.995762Z","shell.execute_reply":"2024-10-22T05:16:27.107885Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\n# Identify numerical and categorical columns for `train_with_bureau`\nnumerical_cols_train_with_bureau = train_with_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_train_with_bureau = train_with_bureau.select_dtypes(include=['object']).columns\n\n# Identify numerical and categorical columns for `train_without_bureau`\nnumerical_cols_train_without_bureau = train_without_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_train_without_bureau = train_without_bureau.select_dtypes(include=['object']).columns\n\n# Identify numerical and categorical columns for `test_with_bureau`\nnumerical_cols_test_with_bureau = test_with_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_test_with_bureau = test_with_bureau.select_dtypes(include=['object']).columns\n\n# Identify numerical and categorical columns for `test_without_bureau`\nnumerical_cols_test_without_bureau = test_without_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_test_without_bureau = test_without_bureau.select_dtypes(include=['object']).columns\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:27.111064Z","iopub.execute_input":"2024-10-22T05:16:27.111495Z","iopub.status.idle":"2024-10-22T05:16:28.138927Z","shell.execute_reply.started":"2024-10-22T05:16:27.111424Z","shell.execute_reply":"2024-10-22T05:16:28.138088Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Convert categorical columns to numeric if possible\ndef convert_to_numeric(df, col):\n    try:\n        df[col] = pd.to_numeric(df[col])  # 'coerce' will replace invalid parsing with NaN\n    except ValueError:\n        pass\n\n# For `train_with_bureau`\nfor col in categorical_cols_train_with_bureau:\n    convert_to_numeric(train_with_bureau, col)\n\n# For `train_without_bureau`\nfor col in categorical_cols_train_without_bureau:\n    convert_to_numeric(train_without_bureau, col)\n\n# For `test_with_bureau`\nfor col in categorical_cols_test_with_bureau:\n    convert_to_numeric(test_with_bureau, col)\n\n# For `test_without_bureau`\nfor col in categorical_cols_test_without_bureau:\n    convert_to_numeric(test_without_bureau, col)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:28.140085Z","iopub.execute_input":"2024-10-22T05:16:28.140429Z","iopub.status.idle":"2024-10-22T05:16:28.286928Z","shell.execute_reply.started":"2024-10-22T05:16:28.140393Z","shell.execute_reply":"2024-10-22T05:16:28.286211Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport ast\n\n# Function to convert string representations to lists\ndef convert_to_list(value):\n    if pd.isna(value):\n        return np.nan\n    try:\n        value = value.strip('\"')  # Remove quotes\n        return ast.literal_eval(value)  # Convert string to list\n    except (ValueError, SyntaxError):\n        return np.nan\n\n# Handle NaN by converting floats to empty lists\ndef handle_NaN(value):\n    if isinstance(value, float):\n        return []  # Convert NaN to empty list for consistency\n    return value\n\n# Columns to process (specific to datasets with bureau data)\ncolumns_to_convert = ['add_671_x', 'add_671_y']\n\n# Apply transformations to `train_with_bureau`\nfor col in columns_to_convert:\n    train_with_bureau[col] = train_with_bureau[col].apply(convert_to_list)\n    train_with_bureau[col] = train_with_bureau[col].apply(handle_NaN)\n\n# Apply transformations to `test_with_bureau`\nfor col in columns_to_convert:\n    test_with_bureau[col] = test_with_bureau[col].apply(convert_to_list)\n    test_with_bureau[col] = test_with_bureau[col].apply(handle_NaN)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:28.288599Z","iopub.execute_input":"2024-10-22T05:16:28.288876Z","iopub.status.idle":"2024-10-22T05:16:30.726837Z","shell.execute_reply.started":"2024-10-22T05:16:28.288838Z","shell.execute_reply":"2024-10-22T05:16:30.725879Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Function to flatten list columns and create separate features\ndef flatten_lists(df1, df2, col):\n    # Get the maximum length of the lists in both DataFrames\n    max_length = df1[col].apply(len).max()\n    max_length = max(max_length, df2[col].apply(len).max())\n    # Iterate through the max length and create new columns for each element in the list\n    for i in range(max_length):\n        name = col + \"_\" + str(i)\n        # For df1 (train_with_bureau)\n        df1[name] = df1[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n        convert_to_numeric(df1, name)\n        # For df2 (test_with_bureau)\n        df2[name] = df2[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n        convert_to_numeric(df2, name)\n    # Drop the original list column as it has been flattened\n    df1.drop(columns=[col], inplace=True)\n    df2.drop(columns=[col], inplace=True)\n    return max_length\n\n# Apply flatten_lists only to datasets with bureau data\nflatten_lists(train_with_bureau, test_with_bureau, 'add_671_x')\nflatten_lists(train_with_bureau, test_with_bureau, 'add_671_y')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:30.728007Z","iopub.execute_input":"2024-10-22T05:16:30.728293Z","iopub.status.idle":"2024-10-22T05:16:31.954500Z","shell.execute_reply.started":"2024-10-22T05:16:30.728263Z","shell.execute_reply":"2024-10-22T05:16:31.953603Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"# Identify numerical and categorical columns for each dataset\n\n# Train with bureau data\nnumerical_cols_train_with_bureau = train_with_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_train_with_bureau = train_with_bureau.select_dtypes(include=['object']).columns\n\n# Train without bureau data\nnumerical_cols_train_without_bureau = train_without_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_train_without_bureau = train_without_bureau.select_dtypes(include=['object']).columns\n\n# Test with bureau data\nnumerical_cols_test_with_bureau = test_with_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_test_with_bureau = test_with_bureau.select_dtypes(include=['object']).columns\n\n# Test without bureau data\nnumerical_cols_test_without_bureau = test_without_bureau.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_test_without_bureau = test_without_bureau.select_dtypes(include=['object']).columns\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:31.955627Z","iopub.execute_input":"2024-10-22T05:16:31.955927Z","iopub.status.idle":"2024-10-22T05:16:33.057486Z","shell.execute_reply.started":"2024-10-22T05:16:31.955895Z","shell.execute_reply":"2024-10-22T05:16:33.056697Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# # Print categorical columns and distinct values for each dataset\n\n# # Train with bureau\n# print('Categorical columns (train_with_bureau): ', categorical_cols_train_with_bureau)\n# print(\"\\nDistinct values in categorical columns (train_with_bureau):\")\n# for col in categorical_cols_train_with_bureau:\n#     print(f\"{col}: {train_with_bureau[col].nunique()}\")\n\n# # Train without bureau\n# print('Categorical columns (train_without_bureau): ', categorical_cols_train_without_bureau)\n# print(\"\\nDistinct values in categorical columns (train_without_bureau):\")\n# for col in categorical_cols_train_without_bureau:\n#     print(f\"{col}: {train_without_bureau[col].nunique()}\")\n\n# # Test with bureau\n# print('Categorical columns (test_with_bureau): ', categorical_cols_test_with_bureau)\n# print(\"\\nDistinct values in categorical columns (test_with_bureau):\")\n# for col in categorical_cols_test_with_bureau:\n#     print(f\"{col}: {test_with_bureau[col].nunique()}\")\n\n# # Test without bureau\n# print('Categorical columns (test_without_bureau): ', categorical_cols_test_without_bureau)\n# print(\"\\nDistinct values in categorical columns (test_without_bureau):\")\n# for col in categorical_cols_test_without_bureau:\n#     print(f\"{col}: {test_without_bureau[col].nunique()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:33.059407Z","iopub.execute_input":"2024-10-22T05:16:33.059741Z","iopub.status.idle":"2024-10-22T05:16:33.064758Z","shell.execute_reply.started":"2024-10-22T05:16:33.059707Z","shell.execute_reply":"2024-10-22T05:16:33.063870Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer_num = SimpleImputer(strategy='mean')\nimputer_cat = SimpleImputer(strategy='constant', fill_value='NA')\n\ndfs = [train_with_bureau, train_without_bureau, test_with_bureau, test_without_bureau]\n\nfor df in dfs:\n    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    cat_cols = df.select_dtypes(include=['object']).columns\n    \n    df[num_cols] = imputer_num.fit_transform(df[num_cols])\n    df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:16:33.066004Z","iopub.execute_input":"2024-10-22T05:16:33.066275Z","iopub.status.idle":"2024-10-22T05:16:39.304885Z","shell.execute_reply.started":"2024-10-22T05:16:33.066245Z","shell.execute_reply":"2024-10-22T05:16:39.304044Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# # Release memory by deleting unnecessary variables and forcing garbage collection\n# del  test_2_1, test_2_2, train_1, train_2_1, train_2_2\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:09:25.393396Z","iopub.execute_input":"2024-10-22T05:09:25.393825Z","iopub.status.idle":"2024-10-22T05:09:25.398738Z","shell.execute_reply.started":"2024-10-22T05:09:25.393779Z","shell.execute_reply":"2024-10-22T05:09:25.397763Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Select relevant features for train_with_bureau\nfeatures_with_bureau = train_with_bureau.columns.drop(['id', 'label', 'loan_id'])\nX_train_with_bureau = train_with_bureau[features_with_bureau]\ny_train_with_bureau = train_with_bureau['label']\n\n# Select relevant features for train_without_bureau\nfeatures_without_bureau = train_without_bureau.columns.drop(['id', 'label', 'loan_id'])\nX_train_without_bureau = train_without_bureau[features_without_bureau]\ny_train_without_bureau = train_without_bureau['label']\n\n# Select relevant features for test_with_bureau\nfeatures_test_with_bureau = test_with_bureau.columns.drop(['id', 'loan_id'])\nX_test_with_bureau = test_with_bureau[features_test_with_bureau]\n\n# Select relevant features for test_without_bureau\nfeatures_test_without_bureau = test_without_bureau.columns.drop(['id', 'loan_id'])\nX_test_without_bureau = test_without_bureau[features_test_without_bureau]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:17:29.220624Z","iopub.execute_input":"2024-10-22T05:17:29.221276Z","iopub.status.idle":"2024-10-22T05:17:29.947158Z","shell.execute_reply.started":"2024-10-22T05:17:29.221234Z","shell.execute_reply":"2024-10-22T05:17:29.946168Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# import gc\n\n# # Release memory by deleting unnecessary variables\n# del train_with_bureau, train_without_bureau, test_with_bureau, test_without_bureau\n\n# # Force garbage collection\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:44:37.115445Z","iopub.execute_input":"2024-10-21T18:44:37.115795Z","iopub.status.idle":"2024-10-21T18:44:37.120454Z","shell.execute_reply.started":"2024-10-21T18:44:37.115761Z","shell.execute_reply":"2024-10-21T18:44:37.119229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Function to sanitize column names by removing special characters\ndef sanitize_column_names(df):\n    # Replace or remove invalid characters (anything not a letter, number, or underscore)\n    df.columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n    return df\n\n# Function to process datasets\ndef process_dataset(X_train, X_test, categorical_cols):\n    # Remove 'loan_id' and 'id' from the list of categorical columns if they exist\n    categorical_cols = [col for col in categorical_cols if col not in ['loan_id', 'id']]\n\n    # Remove 'loan_id' and 'id' from the features if they exist\n    X_train = X_train.drop(columns=['loan_id', 'id'], errors='ignore')\n    X_test = X_test.drop(columns=['loan_id', 'id'], errors='ignore')\n\n    # Convert categorical columns to numerical using one-hot encoding\n    X_train = pd.get_dummies(X_train, columns=categorical_cols)\n    X_test = pd.get_dummies(X_test, columns=categorical_cols)\n\n    # Apply sanitization to both train and test datasets\n    X_train = sanitize_column_names(X_train)\n    X_test = sanitize_column_names(X_test)\n\n    # Ensure the same columns in train and test after one-hot encoding\n    X_train, X_test = X_train.align(X_test, join='inner', axis=1, fill_value=0)\n\n    return X_train, X_test\n\n# Process datasets with bureau data\nX_train_with_bureau, X_test_with_bureau = process_dataset(X_train_with_bureau, X_test_with_bureau, categorical_cols_train_with_bureau)\n\n# Process datasets without bureau data\nX_train_without_bureau, X_test_without_bureau = process_dataset(X_train_without_bureau, X_test_without_bureau, categorical_cols_train_without_bureau)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T05:17:32.998682Z","iopub.execute_input":"2024-10-22T05:17:32.999047Z","iopub.status.idle":"2024-10-22T05:17:36.870429Z","shell.execute_reply.started":"2024-10-22T05:17:32.999014Z","shell.execute_reply":"2024-10-22T05:17:36.869592Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\n# Function to train the model\ndef train_model(X_train, y_train):\n    print(f\"Trainig with {X_train}\")\n    \n    # Split data for validation\n    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n    model = xgb.XGBClassifier(\n        booster='dart',\n        reg_lambda=0.00005633433783615297,\n        alpha=0.000017886579960049807,\n        subsample=0.7695036259589584,\n        colsample_bytree=0.6145464426199919,\n        max_depth=6,\n        learning_rate=0.01,\n        n_estimators=637,\n        min_child_weight=9,\n        gamma=0.34168672204351896,\n        eval_metric='auc',\n        random_state=42,\n        tree_method='hist',\n        device='cuda'\n    )\n\n    model.fit(\n        X_train_split, y_train_split,\n        eval_set=[(X_val, y_val)],\n        verbose=True\n    )\n    \n    return model\n\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-22T05:37:29.445758Z","iopub.execute_input":"2024-10-22T05:37:29.446630Z","iopub.status.idle":"2024-10-22T05:37:29.454960Z","shell.execute_reply.started":"2024-10-22T05:37:29.446580Z","shell.execute_reply":"2024-10-22T05:37:29.453770Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Train the model with bureau data\nmodel_with_bureau = train_model(X_train_with_bureau, y_train_with_bureau)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-22T05:29:32.574487Z","iopub.execute_input":"2024-10-22T05:29:32.575137Z","iopub.status.idle":"2024-10-22T05:30:49.557561Z","shell.execute_reply.started":"2024-10-22T05:29:32.575089Z","shell.execute_reply":"2024-10-22T05:30:49.556499Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Trainig with           col_1  col_2  col_3  col_4  col_5     col_6     col_7     col_8  \\\n0      0.004214  16.99    0.0    0.0    0.0  0.000766  0.000213  0.003886   \n1      0.004880  28.00    0.0    0.0    0.0  0.001947  0.001069  0.001620   \n2      0.005392  34.00    0.0    0.0    0.0  0.002217  0.001044  0.002108   \n3      0.000901  30.00    0.0    0.0    0.0  0.000253  0.000034  0.001592   \n4      0.000850  36.00    0.0    0.0    0.0  0.000632  0.000052  0.002150   \n...         ...    ...    ...    ...    ...       ...       ...       ...   \n45364  0.141421  14.62    0.0    0.0   37.0  0.205002  0.191226  0.002089   \n45365  0.038488  14.22    0.0    0.0  203.0  0.032655  0.028844  0.013361   \n45366  0.614863  12.47    0.0    0.0  234.0  0.749999  0.698917  0.010655   \n45367  0.139209  12.72    0.0    0.0  234.0  0.145601  0.133762  0.003242   \n45368  0.020997  14.22    0.0    0.0  234.0  0.006304  0.004095  0.004451   \n\n          col_9    col_10  ...  add_677_x__  add_677_x_NA  add_672_y__  \\\n0      0.000554  0.000554  ...         True         False         True   \n1      0.000877  0.000877  ...         True         False         True   \n2      0.001174  0.001174  ...         True         False         True   \n3      0.000219  0.000219  ...         True         False         True   \n4      0.000580  0.000580  ...         True         False         True   \n...         ...       ...  ...          ...           ...          ...   \n45364  0.013776  0.013776  ...        False          True         True   \n45365  0.003811  0.003811  ...        False          True         True   \n45366  0.051083  0.051083  ...        False          True         True   \n45367  0.011839  0.011839  ...        False          True         True   \n45368  0.002210  0.002210  ...        False          True         True   \n\n       add_673_y__  add_675_y__  add_675_y_NA  add_676_y__  add_676_y_NA  \\\n0             True        False          True        False          True   \n1             True        False          True        False          True   \n2             True        False          True        False          True   \n3             True        False          True        False          True   \n4             True        False          True        False          True   \n...            ...          ...           ...          ...           ...   \n45364         True         True         False         True         False   \n45365         True         True         False         True         False   \n45366         True         True         False         True         False   \n45367         True         True         False         True         False   \n45368         True         True         False         True         False   \n\n       add_677_y__  add_677_y_NA  \n0            False          True  \n1            False          True  \n2            False          True  \n3            False          True  \n4            False          True  \n...            ...           ...  \n45364         True         False  \n45365         True         False  \n45366         True         False  \n45367         True         False  \n45368         True         False  \n\n[45369 rows x 1615 columns]\n[0]\tvalidation_0-auc:0.73070\n[1]\tvalidation_0-auc:0.78493\n[2]\tvalidation_0-auc:0.80481\n[3]\tvalidation_0-auc:0.80826\n[4]\tvalidation_0-auc:0.81648\n[5]\tvalidation_0-auc:0.82285\n[6]\tvalidation_0-auc:0.82503\n[7]\tvalidation_0-auc:0.82746\n[8]\tvalidation_0-auc:0.83008\n[9]\tvalidation_0-auc:0.83124\n[10]\tvalidation_0-auc:0.83104\n[11]\tvalidation_0-auc:0.83147\n[12]\tvalidation_0-auc:0.83188\n[13]\tvalidation_0-auc:0.83364\n[14]\tvalidation_0-auc:0.83379\n[15]\tvalidation_0-auc:0.83355\n[16]\tvalidation_0-auc:0.83225\n[17]\tvalidation_0-auc:0.83332\n[18]\tvalidation_0-auc:0.83354\n[19]\tvalidation_0-auc:0.83276\n[20]\tvalidation_0-auc:0.83314\n[21]\tvalidation_0-auc:0.83676\n[22]\tvalidation_0-auc:0.83703\n[23]\tvalidation_0-auc:0.83839\n[24]\tvalidation_0-auc:0.83873\n[25]\tvalidation_0-auc:0.83868\n[26]\tvalidation_0-auc:0.83828\n[27]\tvalidation_0-auc:0.83873\n[28]\tvalidation_0-auc:0.83892\n[29]\tvalidation_0-auc:0.83894\n[30]\tvalidation_0-auc:0.83896\n[31]\tvalidation_0-auc:0.83929\n[32]\tvalidation_0-auc:0.83919\n[33]\tvalidation_0-auc:0.83930\n[34]\tvalidation_0-auc:0.84038\n[35]\tvalidation_0-auc:0.84019\n[36]\tvalidation_0-auc:0.84090\n[37]\tvalidation_0-auc:0.84160\n[38]\tvalidation_0-auc:0.84029\n[39]\tvalidation_0-auc:0.84088\n[40]\tvalidation_0-auc:0.84043\n[41]\tvalidation_0-auc:0.84091\n[42]\tvalidation_0-auc:0.84090\n[43]\tvalidation_0-auc:0.84138\n[44]\tvalidation_0-auc:0.84174\n[45]\tvalidation_0-auc:0.84136\n[46]\tvalidation_0-auc:0.84264\n[47]\tvalidation_0-auc:0.84265\n[48]\tvalidation_0-auc:0.84289\n[49]\tvalidation_0-auc:0.84283\n[50]\tvalidation_0-auc:0.84271\n[51]\tvalidation_0-auc:0.84311\n[52]\tvalidation_0-auc:0.84363\n[53]\tvalidation_0-auc:0.84382\n[54]\tvalidation_0-auc:0.84388\n[55]\tvalidation_0-auc:0.84400\n[56]\tvalidation_0-auc:0.84404\n[57]\tvalidation_0-auc:0.84402\n[58]\tvalidation_0-auc:0.84456\n[59]\tvalidation_0-auc:0.84492\n[60]\tvalidation_0-auc:0.84460\n[61]\tvalidation_0-auc:0.84471\n[62]\tvalidation_0-auc:0.84510\n[63]\tvalidation_0-auc:0.84508\n[64]\tvalidation_0-auc:0.84532\n[65]\tvalidation_0-auc:0.84522\n[66]\tvalidation_0-auc:0.84548\n[67]\tvalidation_0-auc:0.84606\n[68]\tvalidation_0-auc:0.84606\n[69]\tvalidation_0-auc:0.84635\n[70]\tvalidation_0-auc:0.84651\n[71]\tvalidation_0-auc:0.84669\n[72]\tvalidation_0-auc:0.84676\n[73]\tvalidation_0-auc:0.84689\n[74]\tvalidation_0-auc:0.84721\n[75]\tvalidation_0-auc:0.84821\n[76]\tvalidation_0-auc:0.84860\n[77]\tvalidation_0-auc:0.84844\n[78]\tvalidation_0-auc:0.84907\n[79]\tvalidation_0-auc:0.84905\n[80]\tvalidation_0-auc:0.84903\n[81]\tvalidation_0-auc:0.84904\n[82]\tvalidation_0-auc:0.84934\n[83]\tvalidation_0-auc:0.84952\n[84]\tvalidation_0-auc:0.84917\n[85]\tvalidation_0-auc:0.84920\n[86]\tvalidation_0-auc:0.84945\n[87]\tvalidation_0-auc:0.84970\n[88]\tvalidation_0-auc:0.85022\n[89]\tvalidation_0-auc:0.85026\n[90]\tvalidation_0-auc:0.85063\n[91]\tvalidation_0-auc:0.85055\n[92]\tvalidation_0-auc:0.85079\n[93]\tvalidation_0-auc:0.85081\n[94]\tvalidation_0-auc:0.85084\n[95]\tvalidation_0-auc:0.85055\n[96]\tvalidation_0-auc:0.85093\n[97]\tvalidation_0-auc:0.85061\n[98]\tvalidation_0-auc:0.85105\n[99]\tvalidation_0-auc:0.85144\n[100]\tvalidation_0-auc:0.85163\n[101]\tvalidation_0-auc:0.85212\n[102]\tvalidation_0-auc:0.85241\n[103]\tvalidation_0-auc:0.85263\n[104]\tvalidation_0-auc:0.85287\n[105]\tvalidation_0-auc:0.85309\n[106]\tvalidation_0-auc:0.85333\n[107]\tvalidation_0-auc:0.85377\n[108]\tvalidation_0-auc:0.85402\n[109]\tvalidation_0-auc:0.85423\n[110]\tvalidation_0-auc:0.85450\n[111]\tvalidation_0-auc:0.85453\n[112]\tvalidation_0-auc:0.85478\n[113]\tvalidation_0-auc:0.85492\n[114]\tvalidation_0-auc:0.85510\n[115]\tvalidation_0-auc:0.85516\n[116]\tvalidation_0-auc:0.85520\n[117]\tvalidation_0-auc:0.85540\n[118]\tvalidation_0-auc:0.85543\n[119]\tvalidation_0-auc:0.85555\n[120]\tvalidation_0-auc:0.85568\n[121]\tvalidation_0-auc:0.85578\n[122]\tvalidation_0-auc:0.85582\n[123]\tvalidation_0-auc:0.85621\n[124]\tvalidation_0-auc:0.85635\n[125]\tvalidation_0-auc:0.85631\n[126]\tvalidation_0-auc:0.85629\n[127]\tvalidation_0-auc:0.85633\n[128]\tvalidation_0-auc:0.85689\n[129]\tvalidation_0-auc:0.85719\n[130]\tvalidation_0-auc:0.85717\n[131]\tvalidation_0-auc:0.85732\n[132]\tvalidation_0-auc:0.85691\n[133]\tvalidation_0-auc:0.85722\n[134]\tvalidation_0-auc:0.85704\n[135]\tvalidation_0-auc:0.85726\n[136]\tvalidation_0-auc:0.85741\n[137]\tvalidation_0-auc:0.85766\n[138]\tvalidation_0-auc:0.85806\n[139]\tvalidation_0-auc:0.85840\n[140]\tvalidation_0-auc:0.85864\n[141]\tvalidation_0-auc:0.85900\n[142]\tvalidation_0-auc:0.85922\n[143]\tvalidation_0-auc:0.85919\n[144]\tvalidation_0-auc:0.85923\n[145]\tvalidation_0-auc:0.85933\n[146]\tvalidation_0-auc:0.85967\n[147]\tvalidation_0-auc:0.85983\n[148]\tvalidation_0-auc:0.85996\n[149]\tvalidation_0-auc:0.86021\n[150]\tvalidation_0-auc:0.86031\n[151]\tvalidation_0-auc:0.86024\n[152]\tvalidation_0-auc:0.86024\n[153]\tvalidation_0-auc:0.86013\n[154]\tvalidation_0-auc:0.86012\n[155]\tvalidation_0-auc:0.85989\n[156]\tvalidation_0-auc:0.86019\n[157]\tvalidation_0-auc:0.86009\n[158]\tvalidation_0-auc:0.86020\n[159]\tvalidation_0-auc:0.86019\n[160]\tvalidation_0-auc:0.86019\n[161]\tvalidation_0-auc:0.86031\n[162]\tvalidation_0-auc:0.86041\n[163]\tvalidation_0-auc:0.86058\n[164]\tvalidation_0-auc:0.86055\n[165]\tvalidation_0-auc:0.86065\n[166]\tvalidation_0-auc:0.86087\n[167]\tvalidation_0-auc:0.86108\n[168]\tvalidation_0-auc:0.86114\n[169]\tvalidation_0-auc:0.86124\n[170]\tvalidation_0-auc:0.86139\n[171]\tvalidation_0-auc:0.86138\n[172]\tvalidation_0-auc:0.86143\n[173]\tvalidation_0-auc:0.86153\n[174]\tvalidation_0-auc:0.86151\n[175]\tvalidation_0-auc:0.86158\n[176]\tvalidation_0-auc:0.86158\n[177]\tvalidation_0-auc:0.86154\n[178]\tvalidation_0-auc:0.86158\n[179]\tvalidation_0-auc:0.86154\n[180]\tvalidation_0-auc:0.86171\n[181]\tvalidation_0-auc:0.86162\n[182]\tvalidation_0-auc:0.86173\n[183]\tvalidation_0-auc:0.86194\n[184]\tvalidation_0-auc:0.86213\n[185]\tvalidation_0-auc:0.86203\n[186]\tvalidation_0-auc:0.86216\n[187]\tvalidation_0-auc:0.86205\n[188]\tvalidation_0-auc:0.86205\n[189]\tvalidation_0-auc:0.86236\n[190]\tvalidation_0-auc:0.86264\n[191]\tvalidation_0-auc:0.86266\n[192]\tvalidation_0-auc:0.86270\n[193]\tvalidation_0-auc:0.86290\n[194]\tvalidation_0-auc:0.86294\n[195]\tvalidation_0-auc:0.86300\n[196]\tvalidation_0-auc:0.86303\n[197]\tvalidation_0-auc:0.86331\n[198]\tvalidation_0-auc:0.86334\n[199]\tvalidation_0-auc:0.86356\n[200]\tvalidation_0-auc:0.86352\n[201]\tvalidation_0-auc:0.86353\n[202]\tvalidation_0-auc:0.86351\n[203]\tvalidation_0-auc:0.86364\n[204]\tvalidation_0-auc:0.86368\n[205]\tvalidation_0-auc:0.86382\n[206]\tvalidation_0-auc:0.86391\n[207]\tvalidation_0-auc:0.86395\n[208]\tvalidation_0-auc:0.86396\n[209]\tvalidation_0-auc:0.86410\n[210]\tvalidation_0-auc:0.86417\n[211]\tvalidation_0-auc:0.86413\n[212]\tvalidation_0-auc:0.86414\n[213]\tvalidation_0-auc:0.86416\n[214]\tvalidation_0-auc:0.86429\n[215]\tvalidation_0-auc:0.86449\n[216]\tvalidation_0-auc:0.86459\n[217]\tvalidation_0-auc:0.86463\n[218]\tvalidation_0-auc:0.86466\n[219]\tvalidation_0-auc:0.86464\n[220]\tvalidation_0-auc:0.86486\n[221]\tvalidation_0-auc:0.86495\n[222]\tvalidation_0-auc:0.86513\n[223]\tvalidation_0-auc:0.86528\n[224]\tvalidation_0-auc:0.86521\n[225]\tvalidation_0-auc:0.86529\n[226]\tvalidation_0-auc:0.86522\n[227]\tvalidation_0-auc:0.86525\n[228]\tvalidation_0-auc:0.86531\n[229]\tvalidation_0-auc:0.86543\n[230]\tvalidation_0-auc:0.86535\n[231]\tvalidation_0-auc:0.86539\n[232]\tvalidation_0-auc:0.86534\n[233]\tvalidation_0-auc:0.86524\n[234]\tvalidation_0-auc:0.86528\n[235]\tvalidation_0-auc:0.86532\n[236]\tvalidation_0-auc:0.86534\n[237]\tvalidation_0-auc:0.86530\n[238]\tvalidation_0-auc:0.86524\n[239]\tvalidation_0-auc:0.86520\n[240]\tvalidation_0-auc:0.86531\n[241]\tvalidation_0-auc:0.86526\n[242]\tvalidation_0-auc:0.86524\n[243]\tvalidation_0-auc:0.86537\n[244]\tvalidation_0-auc:0.86536\n[245]\tvalidation_0-auc:0.86536\n[246]\tvalidation_0-auc:0.86546\n[247]\tvalidation_0-auc:0.86554\n[248]\tvalidation_0-auc:0.86564\n[249]\tvalidation_0-auc:0.86564\n[250]\tvalidation_0-auc:0.86574\n[251]\tvalidation_0-auc:0.86579\n[252]\tvalidation_0-auc:0.86587\n[253]\tvalidation_0-auc:0.86593\n[254]\tvalidation_0-auc:0.86602\n[255]\tvalidation_0-auc:0.86599\n[256]\tvalidation_0-auc:0.86599\n[257]\tvalidation_0-auc:0.86597\n[258]\tvalidation_0-auc:0.86597\n[259]\tvalidation_0-auc:0.86599\n[260]\tvalidation_0-auc:0.86600\n[261]\tvalidation_0-auc:0.86602\n[262]\tvalidation_0-auc:0.86602\n[263]\tvalidation_0-auc:0.86606\n[264]\tvalidation_0-auc:0.86604\n[265]\tvalidation_0-auc:0.86603\n[266]\tvalidation_0-auc:0.86601\n[267]\tvalidation_0-auc:0.86600\n[268]\tvalidation_0-auc:0.86600\n[269]\tvalidation_0-auc:0.86600\n[270]\tvalidation_0-auc:0.86603\n[271]\tvalidation_0-auc:0.86607\n[272]\tvalidation_0-auc:0.86610\n[273]\tvalidation_0-auc:0.86609\n[274]\tvalidation_0-auc:0.86608\n[275]\tvalidation_0-auc:0.86608\n[276]\tvalidation_0-auc:0.86606\n[277]\tvalidation_0-auc:0.86606\n[278]\tvalidation_0-auc:0.86606\n[279]\tvalidation_0-auc:0.86606\n[280]\tvalidation_0-auc:0.86606\n[281]\tvalidation_0-auc:0.86606\n[282]\tvalidation_0-auc:0.86602\n[283]\tvalidation_0-auc:0.86602\n[284]\tvalidation_0-auc:0.86601\n[285]\tvalidation_0-auc:0.86598\n[286]\tvalidation_0-auc:0.86598\n[287]\tvalidation_0-auc:0.86601\n[288]\tvalidation_0-auc:0.86601\n[289]\tvalidation_0-auc:0.86601\n[290]\tvalidation_0-auc:0.86601\n[291]\tvalidation_0-auc:0.86601\n[292]\tvalidation_0-auc:0.86601\n[293]\tvalidation_0-auc:0.86602\n[294]\tvalidation_0-auc:0.86602\n[295]\tvalidation_0-auc:0.86603\n[296]\tvalidation_0-auc:0.86603\n[297]\tvalidation_0-auc:0.86603\n[298]\tvalidation_0-auc:0.86603\n[299]\tvalidation_0-auc:0.86603\n[300]\tvalidation_0-auc:0.86603\n[301]\tvalidation_0-auc:0.86606\n[302]\tvalidation_0-auc:0.86606\n[303]\tvalidation_0-auc:0.86607\n[304]\tvalidation_0-auc:0.86603\n[305]\tvalidation_0-auc:0.86603\n[306]\tvalidation_0-auc:0.86615\n[307]\tvalidation_0-auc:0.86615\n[308]\tvalidation_0-auc:0.86615\n[309]\tvalidation_0-auc:0.86615\n[310]\tvalidation_0-auc:0.86615\n[311]\tvalidation_0-auc:0.86615\n[312]\tvalidation_0-auc:0.86615\n[313]\tvalidation_0-auc:0.86615\n[314]\tvalidation_0-auc:0.86615\n[315]\tvalidation_0-auc:0.86616\n[316]\tvalidation_0-auc:0.86607\n[317]\tvalidation_0-auc:0.86607\n[318]\tvalidation_0-auc:0.86607\n[319]\tvalidation_0-auc:0.86608\n[320]\tvalidation_0-auc:0.86608\n[321]\tvalidation_0-auc:0.86608\n[322]\tvalidation_0-auc:0.86608\n[323]\tvalidation_0-auc:0.86608\n[324]\tvalidation_0-auc:0.86608\n[325]\tvalidation_0-auc:0.86611\n[326]\tvalidation_0-auc:0.86611\n[327]\tvalidation_0-auc:0.86610\n[328]\tvalidation_0-auc:0.86607\n[329]\tvalidation_0-auc:0.86607\n[330]\tvalidation_0-auc:0.86607\n[331]\tvalidation_0-auc:0.86606\n[332]\tvalidation_0-auc:0.86604\n[333]\tvalidation_0-auc:0.86604\n[334]\tvalidation_0-auc:0.86604\n[335]\tvalidation_0-auc:0.86604\n[336]\tvalidation_0-auc:0.86604\n[337]\tvalidation_0-auc:0.86605\n[338]\tvalidation_0-auc:0.86605\n[339]\tvalidation_0-auc:0.86605\n[340]\tvalidation_0-auc:0.86605\n[341]\tvalidation_0-auc:0.86605\n[342]\tvalidation_0-auc:0.86605\n[343]\tvalidation_0-auc:0.86604\n[344]\tvalidation_0-auc:0.86604\n[345]\tvalidation_0-auc:0.86604\n[346]\tvalidation_0-auc:0.86604\n[347]\tvalidation_0-auc:0.86604\n[348]\tvalidation_0-auc:0.86604\n[349]\tvalidation_0-auc:0.86604\n[350]\tvalidation_0-auc:0.86604\n[351]\tvalidation_0-auc:0.86603\n[352]\tvalidation_0-auc:0.86593\n[353]\tvalidation_0-auc:0.86593\n[354]\tvalidation_0-auc:0.86595\n[355]\tvalidation_0-auc:0.86595\n[356]\tvalidation_0-auc:0.86595\n[357]\tvalidation_0-auc:0.86595\n[358]\tvalidation_0-auc:0.86593\n[359]\tvalidation_0-auc:0.86593\n[360]\tvalidation_0-auc:0.86593\n[361]\tvalidation_0-auc:0.86600\n[362]\tvalidation_0-auc:0.86600\n[363]\tvalidation_0-auc:0.86600\n[364]\tvalidation_0-auc:0.86600\n[365]\tvalidation_0-auc:0.86600\n[366]\tvalidation_0-auc:0.86600\n[367]\tvalidation_0-auc:0.86600\n[368]\tvalidation_0-auc:0.86600\n[369]\tvalidation_0-auc:0.86600\n[370]\tvalidation_0-auc:0.86600\n[371]\tvalidation_0-auc:0.86600\n[372]\tvalidation_0-auc:0.86600\n[373]\tvalidation_0-auc:0.86596\n[374]\tvalidation_0-auc:0.86595\n[375]\tvalidation_0-auc:0.86595\n[376]\tvalidation_0-auc:0.86595\n[377]\tvalidation_0-auc:0.86598\n[378]\tvalidation_0-auc:0.86598\n[379]\tvalidation_0-auc:0.86598\n[380]\tvalidation_0-auc:0.86598\n[381]\tvalidation_0-auc:0.86598\n[382]\tvalidation_0-auc:0.86598\n[383]\tvalidation_0-auc:0.86598\n[384]\tvalidation_0-auc:0.86600\n[385]\tvalidation_0-auc:0.86605\n[386]\tvalidation_0-auc:0.86605\n[387]\tvalidation_0-auc:0.86605\n[388]\tvalidation_0-auc:0.86605\n[389]\tvalidation_0-auc:0.86605\n[390]\tvalidation_0-auc:0.86605\n[391]\tvalidation_0-auc:0.86605\n[392]\tvalidation_0-auc:0.86605\n[393]\tvalidation_0-auc:0.86605\n[394]\tvalidation_0-auc:0.86605\n[395]\tvalidation_0-auc:0.86605\n[396]\tvalidation_0-auc:0.86606\n[397]\tvalidation_0-auc:0.86606\n[398]\tvalidation_0-auc:0.86606\n[399]\tvalidation_0-auc:0.86606\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Train the model without bureau data\nmodel_without_bureau = train_model(X_train_without_bureau, y_train_without_bureau)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to make predictions and create a mapping\ndef make_predictions_with_mapping(model, X_test, loan_ids):\n    # Make predictions on the test set\n    predictions = model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n\n    # Create a dictionary to map loan_id to its prediction\n    prediction_mapping = {loan_id: prediction for loan_id, prediction in zip(loan_ids, predictions)}\n\n    return prediction_mapping\n\n# Make predictions for the test set with bureau data\npredictions_with_bureau_mapping = make_predictions_with_mapping(model_with_bureau, X_test_with_bureau, test_with_bureau['loan_id'])\n\n# Make predictions for the test set without bureau data\npredictions_without_bureau_mapping = make_predictions_with_mapping(model_without_bureau, X_test_without_bureau, test_without_bureau['loan_id'])\n\n# Combine the mappings\ncombined_predictions_mapping = {**predictions_with_bureau_mapping, **predictions_without_bureau_mapping}\n\n# Create the submission DataFrame using the combined mapping\nsubmission = pd.DataFrame({\n    'loan_id': combined_predictions_mapping.keys(),\n    'prob': combined_predictions_mapping.values()\n})\n\n# Save the predictions to a single submission file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:47:14.049618Z","iopub.execute_input":"2024-10-21T18:47:14.050094Z","iopub.status.idle":"2024-10-21T18:47:26.619979Z","shell.execute_reply.started":"2024-10-21T18:47:14.050056Z","shell.execute_reply":"2024-10-21T18:47:26.618785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\ndef train_lgb_model(X_train, y_train):\n    print(f\"Training with {X_train.shape} shaped data\")\n    \n    # Split data for validation\n    X_train_split, X_val, y_train_split, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42\n    )\n    \n    # Initialize LGBMClassifier with specified parameters\n    model = LGBMClassifier(\n        device='gpu',\n        gpu_platform_id=0,\n        gpu_device_id=0,\n        n_estimators=1000,\n        learning_rate=0.01,\n        num_leaves=50,\n        max_depth= 12,\n        min_child_samples=30,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        # Additional parameters that might be useful\n        boosting_type='gbdt',\n        objective='binary',\n        metric='auc',\n        \n    )\n    \n    # Train the model with validation set\n    model.fit(\n        X_train_split, \n        y_train_split,\n        eval_set=[(X_train_split, y_train_split), (X_val, y_val)],\n        eval_names=['train', 'valid'],\n        eval_metric='auc',\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=50),\n            lgb.callback.log_evaluation(period=100)\n        ]\n    )\n    \n#     # Get validation score\n#     val_preds = model.predict_proba(X_val)[:, 1]\n#     val_score = model.best_score_['valid_0']['auc']  # Extract the specific value\n#     print(f\"\\nBest validation AUC: {val_score:.4f}\")\n\n    # Return the model and validation score\n    return model, val_score\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:15:19.916886Z","iopub.execute_input":"2024-10-22T06:15:19.917678Z","iopub.status.idle":"2024-10-22T06:15:19.927288Z","shell.execute_reply.started":"2024-10-22T06:15:19.917632Z","shell.execute_reply":"2024-10-22T06:15:19.926308Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# Train the model with bureau data\nmodel_with_bureau_lgb = train_lgb_model(X_train_with_bureau, y_train_with_bureau)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:15:22.527360Z","iopub.execute_input":"2024-10-22T06:15:22.527786Z","iopub.status.idle":"2024-10-22T06:16:13.926624Z","shell.execute_reply.started":"2024-10-22T06:15:22.527745Z","shell.execute_reply":"2024-10-22T06:16:13.925299Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Training with (45369, 1615) shaped data\n[LightGBM] [Info] Number of positive: 1925, number of negative: 34370\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 116616\n[LightGBM] [Info] Number of data points in the train set: 36295, number of used features: 1005\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 617 dense feature groups (21.46 MB) transferred to GPU in 0.018761 secs. 1 sparse feature groups\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053038 -> initscore=-2.882258\n[LightGBM] [Info] Start training from score -2.882258\nTraining until validation scores don't improve for 50 rounds\n[100]\ttrain's auc: 0.908995\tvalid's auc: 0.838428\n[200]\ttrain's auc: 0.951222\tvalid's auc: 0.854506\n[300]\ttrain's auc: 0.971293\tvalid's auc: 0.864423\n[400]\ttrain's auc: 0.9843\tvalid's auc: 0.867056\n[500]\ttrain's auc: 0.991487\tvalid's auc: 0.868919\n[600]\ttrain's auc: 0.994982\tvalid's auc: 0.869857\n[700]\ttrain's auc: 0.997114\tvalid's auc: 0.870743\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[705]\ttrain's auc: 0.997193\tvalid's auc: 0.870845\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model with bureau data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_with_bureau_lgb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lgb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_with_bureau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_with_bureau\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[91], line 45\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get validation score\u001b[39;00m\n\u001b[1;32m     44\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 45\u001b[0m val_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_score_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Extract the specific value\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest validation AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Return the model and validation score\u001b[39;00m\n","\u001b[0;31mKeyError\u001b[0m: 'auc'"],"ename":"KeyError","evalue":"'auc'","output_type":"error"}]},{"cell_type":"code","source":"# Train the model without bureau data\nmodel_without_bureau_lgb = train_lgb_model(X_train_without_bureau, y_train_without_bureau)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-22T06:10:26.476153Z","iopub.execute_input":"2024-10-22T06:10:26.476825Z","iopub.status.idle":"2024-10-22T06:10:38.059597Z","shell.execute_reply.started":"2024-10-22T06:10:26.476775Z","shell.execute_reply":"2024-10-22T06:10:38.058270Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Training with (54631, 254) shaped data\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 1408, number of negative: 42296\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 28876\n[LightGBM] [Info] Number of data points in the train set: 43704, number of used features: 230\n[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (3.50 MB) transferred to GPU in 0.003566 secs. 1 sparse feature groups\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032217 -> initscore=-3.402522\n[LightGBM] [Info] Start training from score -3.402522\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003067 secs. 1 sparse feature groups\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003134 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003527 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003123 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003171 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003110 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003131 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003153 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003102 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003105 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003130 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003151 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003130 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003127 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003258 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003146 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003154 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003148 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003170 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003197 secs. 1 sparse feature groups\n[100]\ttrain's auc: 0.919011\tvalid's auc: 0.873235\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003469 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003407 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003310 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003124 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003232 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003107 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003167 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.004824 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003115 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.78 MB) transferred to GPU in 0.003178 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003116 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003143 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003133 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003234 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003188 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003196 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003184 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003444 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003206 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003139 secs. 1 sparse feature groups\n[200]\ttrain's auc: 0.947231\tvalid's auc: 0.875587\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003282 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003206 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003154 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003163 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003123 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003176 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003159 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003159 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003163 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003162 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003170 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003140 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003387 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003186 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003171 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003148 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003181 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003138 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003162 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003128 secs. 1 sparse feature groups\n[300]\ttrain's auc: 0.966582\tvalid's auc: 0.877795\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003240 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003150 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003161 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003372 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003142 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003153 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003088 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003134 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003337 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003344 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003155 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003258 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003169 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003137 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003160 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003157 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003142 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003235 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003137 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003156 secs. 1 sparse feature groups\n[400]\ttrain's auc: 0.976343\tvalid's auc: 0.878622\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003352 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003120 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003120 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003169 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003150 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003260 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003166 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003173 secs. 1 sparse feature groups\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003186 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003209 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003206 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003149 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.81 MB) transferred to GPU in 0.003360 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003138 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.79 MB) transferred to GPU in 0.003173 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 82 dense feature groups (2.80 MB) transferred to GPU in 0.003120 secs. 1 sparse feature groups\nEarly stopping, best iteration is:\n[427]\ttrain's auc: 0.978693\tvalid's auc: 0.878731\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.8 will be ignored. Current value: bagging_fraction=0.8\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model without bureau data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_without_bureau_lgb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lgb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_without_bureau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_without_bureau\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[86], line 50\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     48\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m val_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbest_score_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest validation AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, val_score\n","\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to collections.OrderedDict.__format__"],"ename":"TypeError","evalue":"unsupported format string passed to collections.OrderedDict.__format__","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\ndef train_rf_model(X_train, y_train):\n    print(f\"Training with {X_train.shape} shaped data\")\n    \n    # Split data for validation\n    X_train_split, X_val, y_train_split, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42\n    )\n    \n    # Initialize RandomForestClassifier with parameters\n    model = RandomForestClassifier(\n        n_estimators=300,\n        max_depth=12,           # Similar to LightGBM\n        min_samples_leaf=30,    # Similar to min_child_samples\n        max_features=0.8,       # Similar to colsample_bytree\n        max_samples=0.8,        # Similar to subsample\n        random_state=42,\n        n_jobs=-1,             # Use all CPU cores\n        class_weight='balanced', # Handle imbalanced datasets\n        # Additional parameters\n        min_samples_split=10,\n        bootstrap=True,\n        verbose=1\n    )\n    \n    # Train the model\n    model.fit(X_train_split, y_train_split)\n    \n    # Calculate validation scores\n    train_preds = model.predict_proba(X_train_split)[:, 1]\n    val_preds = model.predict_proba(X_val)[:, 1]\n    \n    train_score = roc_auc_score(y_train_split, train_preds)\n    val_score = roc_auc_score(y_val, val_preds)\n    \n    print(f\"\\nTraining AUC: {train_score:.4f}\")\n    print(f\"Validation AUC: {val_score:.4f}\")\n    \n#     # Optional: Print feature importances\n#     if hasattr(X_train, 'columns'):\n#         importances = pd.DataFrame({\n#             'feature': X_train.columns,\n#             'importance': model.feature_importances_\n#         })\n#         print(\"\\nTop 10 Most Important Features:\")\n#         print(importances.sort_values('importance', ascending=False).head(10))\n    \n    return model, val_score\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:42:13.012385Z","iopub.execute_input":"2024-10-22T06:42:13.013501Z","iopub.status.idle":"2024-10-22T06:42:13.024488Z","shell.execute_reply.started":"2024-10-22T06:42:13.013434Z","shell.execute_reply":"2024-10-22T06:42:13.023042Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nrf_model, rf_score = train_rf_model(X_train_with_bureau, y_train_with_bureau)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T06:42:15.651857Z","iopub.execute_input":"2024-10-22T06:42:15.652714Z","iopub.status.idle":"2024-10-22T06:47:35.168329Z","shell.execute_reply.started":"2024-10-22T06:42:15.652672Z","shell.execute_reply":"2024-10-22T06:47:35.167414Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Training with (45369, 1615) shaped data\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.5min\n[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  5.3min finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed:    0.3s finished\n","output_type":"stream"},{"name":"stdout","text":"\nTraining AUC: 0.9634\nValidation AUC: 0.8438\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed:    0.1s finished\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to make predictions\ndef make_predictions(model, X_test):\n    # Make predictions on the test set\n    return model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n\n# Make predictions for the test set with bureau data\npredictions_with_bureau = make_predictions(model_with_bureau, X_test_with_bureau)\n\n# Make predictions for the test set without bureau data\npredictions_without_bureau = make_predictions(model_without_bureau, X_test_without_bureau)\n# Combine loan_ids from both test sets\nloan_ids = pd.concat([test_with_bureau['loan_id'], test_without_bureau['loan_id']], ignore_index=True)\n\n# Combine predictions from both models\npredictions = np.concatenate([predictions_with_bureau, predictions_without_bureau])\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'loan_id': loan_ids,\n    'prob': predictions\n})\n# Save the predictions to a single submission file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:55:58.319620Z","iopub.execute_input":"2024-10-21T17:55:58.320015Z","iopub.status.idle":"2024-10-21T17:56:10.637937Z","shell.execute_reply.started":"2024-10-21T17:55:58.319980Z","shell.execute_reply":"2024-10-21T17:56:10.636926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the number of rows\nnum_rows = submission.shape[0]\n\nprint(\"Number of rows in the DataFrame:\", num_rows)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T18:00:23.734677Z","iopub.execute_input":"2024-10-21T18:00:23.735754Z","iopub.status.idle":"2024-10-21T18:00:23.740486Z","shell.execute_reply.started":"2024-10-21T18:00:23.735711Z","shell.execute_reply":"2024-10-21T18:00:23.739451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Split data for validation\nX_train_with_bureau, X_valid_with_bureau, y_train_with_bureau, y_valid_with_bureau = train_test_split(\n    X_train_with_bureau, y_train_with_bureau, test_size=0.2, random_state=42\n)\n\nX_train_without_bureau, X_valid_without_bureau, y_train_without_bureau, y_valid_without_bureau = train_test_split(\n    X_train_without_bureau, y_train_without_bureau, test_size=0.2, random_state=42\n)\n\n# Objective function for Optuna\ndef objective(trial, X_train, y_train, X_valid, y_valid):\n    # Sample hyperparameters\n    param = {\n        'verbosity': 0,\n        'objective': 'binary:logistic',\n        # 'tree_method': 'hist',  # Use GPU\n        # 'device': 'cuda',       # Use GPU\n        'eval_metric': 'auc',\n        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 700),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'gamma': trial.suggest_float('gamma', 0, 5)\n    }\n\n    # Create XGBoost DMatrix for train and validation sets\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n    cv_results = xgb.cv(\n        params=param,\n        dtrain=dtrain,\n        nfold=5,  # 5-fold cross-validation\n        num_boost_round= 50,\n        early_stopping_rounds=5,\n        metrics='auc',\n        seed=42,\n        verbose_eval=True\n    )\n    # Extract the best score from cross-validation results\n    mean_auc = cv_results['test-auc-mean'].max()\n    return mean_auc\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T19:03:42.901294Z","iopub.execute_input":"2024-10-21T19:03:42.902040Z","iopub.status.idle":"2024-10-21T19:03:43.164445Z","shell.execute_reply.started":"2024-10-21T19:03:42.901998Z","shell.execute_reply":"2024-10-21T19:03:43.163561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a study object and optimize the objective function\nstudy = optuna.create_study(direction='maximize')  # We want to maximize the ROC AUC score\n\nn_trails = 100\ntimeout = 3600*3\n# Optimize the study for the model with bureau data\nstudy.optimize(lambda trial: objective(trial, X_train_with_bureau, y_train_with_bureau, X_valid_with_bureau, y_valid_with_bureau), n_trials=n_trails, timeout=timeout)\n\n# Get the best hyperparameters for the model with bureau data\nbest_params_with_bureau = study.best_params\nprint(\"Best hyperparameters (with bureau data): \", best_params_with_bureau)\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-21T19:03:53.787231Z","iopub.execute_input":"2024-10-21T19:03:53.788059Z","iopub.status.idle":"2024-10-21T20:27:52.019931Z","shell.execute_reply.started":"2024-10-21T19:03:53.788008Z","shell.execute_reply":"2024-10-21T20:27:52.018018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n# Define the file path\nfile_path = 'best_params_with_bureau.json'\n\n# Write the best hyperparameters to a JSON file\nwith open(file_path, 'w') as file:\n    json.dump(best_params_with_bureau, file, indent=4)\n\nprint(f\"Best hyperparameters saved to {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T20:27:52.401129Z","iopub.execute_input":"2024-10-21T20:27:52.401558Z","iopub.status.idle":"2024-10-21T20:27:52.434332Z","shell.execute_reply.started":"2024-10-21T20:27:52.401508Z","shell.execute_reply":"2024-10-21T20:27:52.433040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset and optimize for the model without bureau data\nstudy = optuna.create_study(direction='maximize')  # Resetting the study for the next optimization\nstudy.optimize(lambda trial: objective(trial, X_train_without_bureau, y_train_without_bureau, X_valid_without_bureau, y_valid_without_bureau), n_trials=n_trails, timeout=timeout)\n\n# Get the best hyperparameters for the model without bureau data\nbest_params_without_bureau = study.best_params\nprint(\"Best hyperparameters (without bureau data): \", best_params_without_bureau)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T20:27:52.435001Z","iopub.status.idle":"2024-10-21T20:27:52.435388Z","shell.execute_reply.started":"2024-10-21T20:27:52.435187Z","shell.execute_reply":"2024-10-21T20:27:52.435218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the file path\nfile_path = 'best_params_without_bureau.json'\n\n# Write the best hyperparameters to a JSON file\nwith open(file_path, 'w') as file:\n    json.dump(best_params_without_bureau, file, indent=4)\n\nprint(f\"Best hyperparameters saved to {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T20:27:52.436776Z","iopub.status.idle":"2024-10-21T20:27:52.437129Z","shell.execute_reply.started":"2024-10-21T20:27:52.436957Z","shell.execute_reply":"2024-10-21T20:27:52.436974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}