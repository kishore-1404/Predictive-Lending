{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117782/176461495.py:8: DtypeWarning: Columns (126,128,143) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_1 = pd.read_csv(data_dir + 'train_1.csv')\n",
      "/tmp/ipykernel_117782/176461495.py:9: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_2_1 = pd.read_csv(data_dir + 'train_2_1.csv')\n",
      "/tmp/ipykernel_117782/176461495.py:10: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_2_2 = pd.read_csv(data_dir + 'train_2_2.csv')\n"
     ]
    }
   ],
   "source": [
    "# Data Directory\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "# # kaggle Data Directory\n",
    "# data_dir = '/kaggle/input/train-test/dataset/'\n",
    "\n",
    "# Load the data\n",
    "train_1 = pd.read_csv(data_dir + 'train_1.csv')\n",
    "train_2_1 = pd.read_csv(data_dir + 'train_2_1.csv')\n",
    "train_2_2 = pd.read_csv(data_dir + 'train_2_2.csv')\n",
    "test_1 = pd.read_csv(data_dir + 'test_1.csv')\n",
    "test_2_1 = pd.read_csv(data_dir + 'test_2_1.csv')\n",
    "test_2_2 = pd.read_csv(data_dir + 'test_2_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on 'id' in the additional information DataFrames\n",
    "train_2_1 = train_2_1.drop_duplicates(subset='id')\n",
    "train_2_2 = train_2_2.drop_duplicates(subset='id')\n",
    "test_2_1 = test_2_1.drop_duplicates(subset='id')\n",
    "test_2_2 = test_2_2.drop_duplicates(subset='id')\n",
    "\n",
    "# Merge the DataFrames\n",
    "train = pd.merge(train_1, train_2_1, on='id', how='left')\n",
    "train = pd.merge(train, train_2_2, on='id', how='left')\n",
    "\n",
    "test = pd.merge(test_1, test_2_1, on='id', how='left')\n",
    "test = pd.merge(test, test_2_2, on='id', how='left')\n",
    "\n",
    "# Replace \"NR\" with NaN\n",
    "train.replace(\"NR\", np.nan, inplace=True)\n",
    "test.replace(\"NR\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols_train = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_train = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_cols_test = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_test = test.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert col to numeric\n",
    "def convert_to_numeric(df, col):\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "for col in categorical_cols_train:\n",
    "    convert_to_numeric(train, col)\n",
    "for col in categorical_cols_test:\n",
    "    convert_to_numeric(test, col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to list\n",
    "import ast\n",
    "\n",
    "def convert_to_list(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    try:\n",
    "        value = value.strip('\"')\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return np.nan\n",
    "def handle_NaN(value):\n",
    "    #check if value is float\n",
    "    if isinstance(value, float):\n",
    "        return []\n",
    "    return value\n",
    "for col in ['add_671_x', 'add_671_y']:\n",
    "    train[col] = train[col].apply(convert_to_list)\n",
    "    test[col] = test[col].apply(convert_to_list)\n",
    "for col in ['add_671_x', 'add_671_y']:\n",
    "    train[col] = train[col].apply(handle_NaN)\n",
    "    test[col] = test[col].apply(handle_NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_lists(df1,df2, col):\n",
    "    max_length = df1[col].apply(len).max()\n",
    "    max_length = max(max_length, df2[col].apply(len).max())\n",
    "    for i in range(max_length):\n",
    "        name = col + \"_\" + str(i)\n",
    "        df1[name] = df1[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "        convert_to_numeric(df1, name)\n",
    "        df2[name] = df2[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "        convert_to_numeric(df2, name)\n",
    "        \n",
    "    df1.drop(columns=[col], inplace=True)\n",
    "    df2.drop(columns=[col], inplace=True)\n",
    "    return max_length\n",
    "flatten_lists(train, test, 'add_671_x')\n",
    "flatten_lists(train, test, 'add_671_y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns train:  Index(['loan_id', 'id', 'prod', 'col_114', 'col_117', 'col_118', 'col_123',\n",
      "       'col_140', 'col_148', 'col_150', 'col_156', 'col_157', 'col_158',\n",
      "       'col_161', 'add_431_x', 'add_672_x', 'add_673_x', 'add_675_x',\n",
      "       'add_676_x', 'add_677_x', 'add_431_y', 'add_672_y', 'add_673_y',\n",
      "       'add_675_y', 'add_676_y', 'add_677_y'],\n",
      "      dtype='object')\n",
      "Categorical columns test:  Index(['loan_id', 'id', 'prod', 'col_114', 'col_117', 'col_118', 'col_123',\n",
      "       'col_140', 'col_148', 'col_150', 'col_156', 'col_157', 'col_158',\n",
      "       'col_161', 'add_431_x', 'add_672_x', 'add_673_x', 'add_675_x',\n",
      "       'add_676_x', 'add_677_x', 'add_431_y', 'add_672_y', 'add_673_y',\n",
      "       'add_675_y', 'add_676_y', 'add_677_y'],\n",
      "      dtype='object')\n",
      "\n",
      "Distinct values in categorical columns (train):\n",
      "loan_id: 100000\n",
      "id: 99584\n",
      "prod: 5\n",
      "col_114: 26\n",
      "col_117: 2\n",
      "col_118: 7\n",
      "col_123: 3\n",
      "col_140: 20\n",
      "col_148: 1\n",
      "col_150: 4\n",
      "col_156: 9\n",
      "col_157: 6\n",
      "col_158: 7\n",
      "col_161: 5\n",
      "add_431_x: 2\n",
      "add_672_x: 1\n",
      "add_673_x: 1\n",
      "add_675_x: 1\n",
      "add_676_x: 1\n",
      "add_677_x: 1\n",
      "add_431_y: 2\n",
      "add_672_y: 1\n",
      "add_673_y: 1\n",
      "add_675_y: 1\n",
      "add_676_y: 1\n",
      "add_677_y: 1\n",
      "\n",
      "Distinct values in categorical columns (test):\n",
      "loan_id: 100000\n",
      "id: 97423\n",
      "prod: 5\n",
      "col_114: 26\n",
      "col_117: 2\n",
      "col_118: 7\n",
      "col_123: 3\n",
      "col_140: 21\n",
      "col_148: 1\n",
      "col_150: 4\n",
      "col_156: 8\n",
      "col_157: 7\n",
      "col_158: 8\n",
      "col_161: 5\n",
      "add_431_x: 4\n",
      "add_672_x: 1\n",
      "add_673_x: 1\n",
      "add_675_x: 1\n",
      "add_676_x: 1\n",
      "add_677_x: 1\n",
      "add_431_y: 4\n",
      "add_672_y: 1\n",
      "add_673_y: 1\n",
      "add_675_y: 1\n",
      "add_676_y: 1\n",
      "add_677_y: 1\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols_train = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_train = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_cols_test = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_test = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "print('Categorical columns train: ', categorical_cols_train)\n",
    "print('Categorical columns test: ', categorical_cols_test)\n",
    "# Print how many distinct values each categorical column has\n",
    "print(\"\\nDistinct values in categorical columns (train):\")\n",
    "for col in categorical_cols_train:\n",
    "    print(f\"{col}: {train[col].nunique()}\")\n",
    "\n",
    "print(\"\\nDistinct values in categorical columns (test):\")\n",
    "for col in categorical_cols_test:\n",
    "    print(f\"{col}: {test[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for numerical columns train\n",
    "for col in numerical_cols_train:\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "# Handle missing values for categorical columns train\n",
    "for col in categorical_cols_train:\n",
    "    train[col] = train[col].fillna('NA')\n",
    "\n",
    "# Handle missing values for numerical columns test\n",
    "for col in numerical_cols_test:\n",
    "    test[col] = test[col].fillna(test[col].mean())\n",
    "# Handle missing values for categorical columns test\n",
    "for col in categorical_cols_test:\n",
    "    test[col] = test[col].fillna('NA')\n",
    "\n",
    "# # Handle outliers for numerical columns using Z-score\n",
    "# for col in numerical_cols_train:\n",
    "#     train = train[(np.abs(stats.zscore(train[col])) < 3)]\n",
    "# for col in numerical_cols_test:\n",
    "#     test = test[(np.abs(stats.zscore(test[col])) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release memory by deleting unnecessary variables and forcing garbage collection\n",
    "del  test_2_1, test_2_2, train_1, train_2_1, train_2_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "# features = train.columns.drop(categorical_cols_train).drop('label')\n",
    "features = train.columns.drop(['id', 'label','loan_id'])\n",
    "X_train = train[features]\n",
    "y_train = train['label']\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release memory by deleting unnecessary variables and forcing garbage collection\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'loan_id' and 'id' from the list of categorical columns if they exist\n",
    "categorical_cols_train = [col for col in categorical_cols_train if col not in ['loan_id', 'id']]\n",
    "categorical_cols_test = [col for col in categorical_cols_test if col not in ['loan_id', 'id']]\n",
    "\n",
    "# Remove 'loan_id' and 'id' from the features if they exist\n",
    "X_train = X_train.drop(columns=['loan_id', 'id'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['loan_id', 'id'], errors='ignore')\n",
    "\n",
    "# Convert categorical columns to numerical using one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols_train)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols_test)\n",
    "\n",
    "# Ensure the same columns in train and test after one-hot encoding\n",
    "X_train, X_test = X_train.align(X_test, join='inner', axis=1, fill_value=0)\n",
    "\n",
    "# Rename columns to ensure they are valid strings without special characters\n",
    "X_train.columns = [str(col).replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_train.columns]\n",
    "X_test.columns = [str(col).replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_test.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Split data for validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the XGBoost model\n",
    "# model = xgb.XGBClassifier(\n",
    "#     n_estimators=1000,        # Increase the number of boosting rounds\n",
    "#     learning_rate=0.05,       # Reduce the learning rate\n",
    "#     max_depth=15,             # Increase the maximum depth of trees\n",
    "#     eval_metric='auc',        # Evaluation metric\n",
    "#     random_state=42           # Seed for reproducibility\n",
    "# )\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_val, y_val)],\n",
    "#     verbose=True\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can then make predictions on the test set\n",
    "# predictions = model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n",
    "\n",
    "# # Ensure the lengths match\n",
    "# # predictions = predictions[:len(test_1)]\n",
    "\n",
    "# # Save predictions to a submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'loan_id': test_1['loan_id'],\n",
    "#     'prob': predictions\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'tree_method': 'hist',  # Use GPU\n",
    "        # 'device': 'cuda',       # Use GPU\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 5000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "\n",
    "    # Create XGBoost DMatrix for train and validation sets\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # # Train the model\n",
    "    # model = xgb.train(param, dtrain, evals=[(dvalid, 'validation')], early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    # # Predict on validation set\n",
    "    # preds = model.predict(dvalid)\n",
    "    \n",
    "    # # Evaluate the model\n",
    "    # auc = roc_auc_score(y_valid, preds)\n",
    "    cv_results = xgb.cv(\n",
    "        params=param,\n",
    "        dtrain=dtrain,\n",
    "        nfold=5,  # 5-fold cross-validation\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=20,\n",
    "        metrics='auc',\n",
    "        seed=42,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    # Extract the best score from cross-validation results\n",
    "    mean_auc = cv_results['test-auc-mean'].max()\n",
    "    return mean_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 22:09:19,412] A new study created in memory with name: no-name-4d22f608-4ab4-443a-890f-56fe74227a77\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize the ROC AUC score\n",
    "\n",
    "# Optimize the study\n",
    "study.optimize(objective, n_trials=100, timeout=3600)  # You can adjust the number of trials and timeout\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Save optimization history plot\n",
    "opt_history = vis.plot_optimization_history(study)\n",
    "opt_history.write_image(\"optuna_optimization_history.png\")\n",
    "\n",
    "# Save hyperparameter importance plot\n",
    "param_importance = vis.plot_param_importances(study)\n",
    "param_importance.write_image(\"optuna_param_importance.png\")\n",
    "\n",
    "# Save parallel coordinate plot\n",
    "parallel_plot = vis.plot_parallel_coordinate(study)\n",
    "parallel_plot.write_image(\"optuna_parallel_coordinate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best hyperparameters to a file\n",
    "\n",
    "with open('best_hyperparams.txt', 'w') as f:\n",
    "    f.write(f\"Best Hyperparameters: {best_params}\\n\")\n",
    "    f.write(f\"Best AUC Score: {study.best_value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['verbosity'] = 0\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'auc'\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "final_model = xgb.train(best_params, dtrain, num_boost_round=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "final_model.save_model('model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test)\n",
    "test_preds = final_model.predict(dtest)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'loan_id': test_1['loan_id'],\n",
    "    'prob': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
