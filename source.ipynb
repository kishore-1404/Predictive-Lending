{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directory\n",
    "data_dir = 'dataset/'\n",
    "\n",
    "# # kaggle Data Directory\n",
    "# data_dir = '/kaggle/input/train-test/dataset/'\n",
    "\n",
    "# Load the data\n",
    "train_1 = pd.read_csv(data_dir + 'train_1.csv')\n",
    "train_2_1 = pd.read_csv(data_dir + 'train_2_1.csv')\n",
    "train_2_2 = pd.read_csv(data_dir + 'train_2_2.csv')\n",
    "test_1 = pd.read_csv(data_dir + 'test_1.csv')\n",
    "test_2_1 = pd.read_csv(data_dir + 'test_2_1.csv')\n",
    "test_2_2 = pd.read_csv(data_dir + 'test_2_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on 'id' in the additional information DataFrames\n",
    "train_2_1 = train_2_1.drop_duplicates(subset='id')\n",
    "train_2_2 = train_2_2.drop_duplicates(subset='id')\n",
    "test_2_1 = test_2_1.drop_duplicates(subset='id')\n",
    "test_2_2 = test_2_2.drop_duplicates(subset='id')\n",
    "\n",
    "# Merge the DataFrames\n",
    "train = pd.merge(train_1, train_2_1, on='id', how='left')\n",
    "train = pd.merge(train, train_2_2, on='id', how='left')\n",
    "\n",
    "test = pd.merge(test_1, test_2_1, on='id', how='left')\n",
    "test = pd.merge(test, test_2_2, on='id', how='left')\n",
    "\n",
    "# Replace \"NR\" with NaN\n",
    "train.replace(\"NR\", np.nan, inplace=True)\n",
    "test.replace(\"NR\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols_train = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_train = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_cols_test = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_test = test.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert col to numeric\n",
    "def convert_to_numeric(df, col):\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "for col in categorical_cols_train:\n",
    "    convert_to_numeric(train, col)\n",
    "for col in categorical_cols_test:\n",
    "    convert_to_numeric(test, col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to list\n",
    "import ast\n",
    "\n",
    "def convert_to_list(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    try:\n",
    "        value = value.strip('\"')\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return np.nan\n",
    "def handle_NaN(value):\n",
    "    #check if value is float\n",
    "    if isinstance(value, float):\n",
    "        return []\n",
    "    return value\n",
    "for col in ['add_671_x', 'add_671_y']:\n",
    "    train[col] = train[col].apply(convert_to_list)\n",
    "    test[col] = test[col].apply(convert_to_list)\n",
    "for col in ['add_671_x', 'add_671_y']:\n",
    "    train[col] = train[col].apply(handle_NaN)\n",
    "    test[col] = test[col].apply(handle_NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lists(df1,df2, col):\n",
    "    max_length = df1[col].apply(len).max()\n",
    "    max_length = max(max_length, df2[col].apply(len).max())\n",
    "    for i in range(max_length):\n",
    "        name = col + \"_\" + str(i)\n",
    "        df1[name] = df1[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "        convert_to_numeric(df1, name)\n",
    "        df2[name] = df2[col].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
    "        convert_to_numeric(df2, name)\n",
    "        \n",
    "    df1.drop(columns=[col], inplace=True)\n",
    "    df2.drop(columns=[col], inplace=True)\n",
    "    return max_length\n",
    "flatten_lists(train, test, 'add_671_x')\n",
    "flatten_lists(train, test, 'add_671_y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols_train = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_train = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_cols_test = test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_test = test.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "print('Categorical columns train: ', categorical_cols_train)\n",
    "print('Categorical columns test: ', categorical_cols_test)\n",
    "# Print how many distinct values each categorical column has\n",
    "print(\"\\nDistinct values in categorical columns (train):\")\n",
    "for col in categorical_cols_train:\n",
    "    print(f\"{col}: {train[col].nunique()}\")\n",
    "\n",
    "print(\"\\nDistinct values in categorical columns (test):\")\n",
    "for col in categorical_cols_test:\n",
    "    print(f\"{col}: {test[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for numerical columns train\n",
    "for col in numerical_cols_train:\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "# Handle missing values for categorical columns train\n",
    "for col in categorical_cols_train:\n",
    "    train[col] = train[col].fillna('NA')\n",
    "\n",
    "# Handle missing values for numerical columns test\n",
    "for col in numerical_cols_test:\n",
    "    test[col] = test[col].fillna(test[col].mean())\n",
    "# Handle missing values for categorical columns test\n",
    "for col in categorical_cols_test:\n",
    "    test[col] = test[col].fillna('NA')\n",
    "\n",
    "# # Handle outliers for numerical columns using Z-score\n",
    "# for col in numerical_cols_train:\n",
    "#     train = train[(np.abs(stats.zscore(train[col])) < 3)]\n",
    "# for col in numerical_cols_test:\n",
    "#     test = test[(np.abs(stats.zscore(test[col])) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory by deleting unnecessary variables and forcing garbage collection\n",
    "del  test_2_1, test_2_2, train_1, train_2_1, train_2_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "# features = train.columns.drop(categorical_cols_train).drop('label')\n",
    "features = train.columns.drop(['id', 'label','loan_id'])\n",
    "X_train = train[features]\n",
    "y_train = train['label']\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory by deleting unnecessary variables and forcing garbage collection\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'loan_id' and 'id' from the list of categorical columns if they exist\n",
    "categorical_cols_train = [col for col in categorical_cols_train if col not in ['loan_id', 'id']]\n",
    "categorical_cols_test = [col for col in categorical_cols_test if col not in ['loan_id', 'id']]\n",
    "\n",
    "# Remove 'loan_id' and 'id' from the features if they exist\n",
    "X_train = X_train.drop(columns=['loan_id', 'id'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['loan_id', 'id'], errors='ignore')\n",
    "\n",
    "# Convert categorical columns to numerical using one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols_train)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols_test)\n",
    "\n",
    "# Ensure the same columns in train and test after one-hot encoding\n",
    "X_train, X_test = X_train.align(X_test, join='inner', axis=1, fill_value=0)\n",
    "\n",
    "# Rename columns to ensure they are valid strings without special characters\n",
    "X_train.columns = [str(col).replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_train.columns]\n",
    "X_test.columns = [str(col).replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in X_test.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from zmq import device\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the XGBoost model\n",
    "# model = xgb.XGBClassifier(\n",
    "#     n_estimators=1000,        # Increase the number of boosting rounds\n",
    "#     learning_rate=0.05,       # Reduce the learning rate\n",
    "#     max_depth=15,             # Increase the maximum depth of trees\n",
    "#     eval_metric='auc',        # Evaluation metric\n",
    "#     random_state=42           # Seed for reproducibility\n",
    "# )\n",
    "model = xgb.XGBClassifier(\n",
    "    reg_lambda=9.462550627558011e-06,\n",
    "    alpha=0.0004394598576628656,\n",
    "    subsample=0.933188231971851,\n",
    "    colsample_bytree=0.5047365676319157,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.019850318615112727,\n",
    "    n_estimators=331,\n",
    "    min_child_weight=8,\n",
    "    gamma=4.346407258134641,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    booster='dart',\n",
    "    tree_method='hist',\n",
    "    device = 'cuda'\n",
    ")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then make predictions on the test set\n",
    "predictions = model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n",
    "\n",
    "# Ensure the lengths match\n",
    "# predictions = predictions[:len(test_1)]\n",
    "\n",
    "# Save predictions to a submission file\n",
    "submission = pd.DataFrame({\n",
    "    'loan_id': test_1['loan_id'],\n",
    "    'prob': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'tree_method': 'hist',  # Use GPU\n",
    "        # 'device': 'cuda',       # Use GPU\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 5000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "\n",
    "    # Create XGBoost DMatrix for train and validation sets\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # # Train the model\n",
    "    # model = xgb.train(param, dtrain, evals=[(dvalid, 'validation')], early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    # # Predict on validation set\n",
    "    # preds = model.predict(dvalid)\n",
    "    \n",
    "    # # Evaluate the model\n",
    "    # auc = roc_auc_score(y_valid, preds)\n",
    "    cv_results = xgb.cv(\n",
    "        params=param,\n",
    "        dtrain=dtrain,\n",
    "        nfold=5,  # 5-fold cross-validation\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=20,\n",
    "        metrics='auc',\n",
    "        seed=42,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    # Extract the best score from cross-validation results\n",
    "    mean_auc = cv_results['test-auc-mean'].max()\n",
    "    return mean_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize the ROC AUC score\n",
    "\n",
    "# Optimize the study\n",
    "study.optimize(objective, n_trials=100, timeout=3600)  # You can adjust the number of trials and timeout\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Save optimization history plot\n",
    "opt_history = vis.plot_optimization_history(study)\n",
    "opt_history.write_image(\"optuna_optimization_history.png\")\n",
    "\n",
    "# Save hyperparameter importance plot\n",
    "param_importance = vis.plot_param_importances(study)\n",
    "param_importance.write_image(\"optuna_param_importance.png\")\n",
    "\n",
    "# Save parallel coordinate plot\n",
    "parallel_plot = vis.plot_parallel_coordinate(study)\n",
    "parallel_plot.write_image(\"optuna_parallel_coordinate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save best hyperparameters to a file\n",
    "\n",
    "with open('best_hyperparams.txt', 'w') as f:\n",
    "    f.write(f\"Best Hyperparameters: {best_params}\\n\")\n",
    "    f.write(f\"Best AUC Score: {study.best_value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params['verbosity'] = 0\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'auc'\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "final_model = xgb.train(best_params, dtrain, num_boost_round=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "final_model.save_model('model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test)\n",
    "test_preds = final_model.predict(dtest)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'loan_id': test_1['loan_id'],\n",
    "    'prob': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
